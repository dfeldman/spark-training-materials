{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Lesson 1: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words did Shakespeare most commonly use? This seems like a simple question, but answering it is a great way to learn the ins and outs of Spark. While you could probably figure this one out by writing a normal program in your language of choice (or even by hand), in order to implement it in a scalable way, you need to use a parallel data processing framework like Spark. \n",
    "\n",
    "The file ../data/shakespeare.txt contains the complete works of Shakespeare. We'll use Spark to find how many times each word was used in 3 different ways. \n",
    "\n",
    "If you haven't used IPython/Jupyter before: The gray boxes contain Python code. You can edit the code by clicking in the box, and then run it by pressing Ctrl-Enter. The output will appear below the box. You can always revert using the Revert option in the file menu. If you want to download this locally, you can get it from github.com/dfeldman/spark-training-materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some necessary helper code (not important to understand, just press Ctrl-Enter to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "import pandas, pandas.tools.plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "try: sc = pyspark.SparkContext('local[*]')\n",
    "except ValueError: pass\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "# Useful function for displaying a DataFrame in a nice-looking way\n",
    "def show(df):\n",
    "   display(HTML(\n",
    "    '<table><tr><th>{}</th></tr><tr>{}</tr></table>'.format(\n",
    "        '</th><th>'.join(str(_) for _ in df.columns),\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in df.take(50))\n",
    "        )\n",
    "     ))\n",
    "\n",
    "\n",
    "def show_rdd(rdd):\n",
    "    show(rdd.toDF())\n",
    "\n",
    "def show_string_rdd(rdd):\n",
    "    show(rdd.map(lambda x: (x,)).toDF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python hints (skip if you are a Python expert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not familiar with Python, here are a few quick tips that will help you get started before we start with Spark.\n",
    "\n",
    "Python has built-in arrays, like many other programming languages. While Spark doesn't use these built-in arrays to store your data, they have some similar properties so it's good to know about arrays first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [\"a\",\"b\", \"c\"]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make an array by splitting a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Here's some text\"\n",
    "text_split = text.split(\" \") # Make a new array by cutting the string at every space\n",
    "print(text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a lot of programming, the way you interact with an array is by using a \"for\" or \"while\" loop to iterate over each element of the array (and possibly change it at each point). **But that is not the right way to use Spark.** Instead, we write small functions that can be applied in parallel to every element of the array, and return a new resulting array all at once. Python has this functionality built in for its arrays too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that returns the input plus 1\n",
    "def xplusone(x):\n",
    "    return x + 1\n",
    "\n",
    "print(xplusone(1))\n",
    "\n",
    "print(list(map(xplusone, [1, 2, 3, 4, 5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code simpler, Python has a way to define a quick, very simple function called a \"lambda function\". This code is exactly the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xplusone = lambda x: x + 1\n",
    "print(xplusone(1))\n",
    "print(list(map(xplusone, [1,2,3,4,5,6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the whole point of a lambda function is that we don't even have to define it in advance. We can just define it when we need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(map(lambda x: x + 1, [1,2,3,4,5,6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this style of programming throughout the tutorial (only using Spark RDDs instead of the built-in Python arrays, and Spark functions instead of the built-in map). The advantage is that Spark will take the function and run it across the entire cluster in parallel, instead of us defining the order in which to execute the function on the elements in the array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other hint: In Python, most objects and functions have built-in documentation. This is very helpful \n",
    "as you're learning. You can see the documentation for an object by running print(<object>.__doc__), and see what's insie an object with print(dir(<object>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(name=\"../data/shakespeare.txt\")\n",
    "print(raw_data.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(raw_data.map.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Word Count Version 1: Using RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are the core data structure in Spark. Everything in Spark is built out of RDDs. \n",
    "This is a solution to the word count problem using only RDDs. RDDs are hard to use though, so in real life you would use DataFrames (next section). \n",
    "\n",
    "RDDs work like a list or array in a traditional programming language. All they do is store a collection of items. You can efficiently apply a function to every element in an RDD using map. \n",
    "\n",
    "Let's create an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(name=\"../data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any point, you can see the contents of an RDD by using the show_string_rdd function that I defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_string_rdd(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first operation will be to split each line into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = raw_data.flatMap(lambda line: line.split(\" \"))\n",
    "show_string_rdd(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll convert all words to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_lower = words.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And filter out empty words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_not_empty = words_lower.filter(lambda x: x != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where it gets a little bit tricky. An RDD is a collection of rows. But right now we just have a collection of individual strings (the words themselves). As a first step to getting a word count, we'll take each word, and turn it into a row of that word and the number 1 attached to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_as_rows = words_not_empty.map(lambda word: (word, 1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. We'll use Spark's reduceByKey function to combine all the word rows, adding up all the attached numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_reduced = words_as_rows.reduceByKey(lambda a, b: a + b)\n",
    "show_rdd(words_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RDD, the first element in each row is the \"key\". In order to sort the RDD by the count of each word, we need to make the count the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_flipped = words_as_rows.map(lambda x: (x[1], x[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_sorted = words_flipped.sortByKey(ascending=False)\n",
    "show_rdd(words_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the whole thing without needing any intermediate lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(name=\"../data/shakespeare.txt\")\n",
    "counts = ((raw_data\n",
    "    .flatMap(lambda line: line.split(\" \"))  # Split each line of text into words\n",
    "    .filter(lambda x: x != \"\")              # Filter out empty words\n",
    "    .map(lambda x: x.lower())               # convert each word to lower case\n",
    "    .map(lambda word: (word, 1))            # Turn each word X into tuple (X, 1)\n",
    "    .reduceByKey(lambda a, b: a + b))       # Count the words\n",
    "    .map(lambda x: (x[1], x[0]))            # Flip the structure (X, Y) to (Y, X) to make sorting easier\n",
    "    .sortByKey(ascending=False))            # Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Word Count Version 2: Using DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you wouldn't use RDDs directly, because Spark provides a much easier-to-use high-level interface called DataFrames. These are similar to Pandas or R frames and provide a lot of built-in functionality for free. Internally, they are built entirely using DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts2 = (raw_data\n",
    "    .flatMap(lambda line: line.split(\" \"))).map(lambda x: (x,)).toDF()\n",
    "\n",
    "df2=counts2.groupBy(\"_1\").count().sort(desc(\"count\"))\n",
    "show(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you modify this to exclude empty words, like the first example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Word Count Version 3: Using SQL\n",
    "\n",
    "Spark supports an even-easier interface, at least for many simple programs: SQL queries. Spark's SQL processing is built on DataFrames, which in turn are built on RDDs. HEre's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts3 = (raw_data\n",
    "    .flatMap(lambda line: line.split(\" \"))).map(lambda x: (x,)).toDF()\n",
    "\n",
    "# Install the \"counts3\" DataFrame as an SQL table\n",
    "counts3.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "df3=spark.sql(\"select lower(_1) as word, count(*) as ct from table1 where _1 != '' group by _1 order by ct desc\")\n",
    "\n",
    "show(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun: graphing the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to get a plot out of Spark is to convert the Spark DataFrame into a Pandas DataFrame (Pandas is another Python library). Pandas has great support for plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pdf = counts.toDF().limit(100).toPandas()\n",
    "pdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "(You can use versions 1, 2, or 3 to answer these. Or for an extreme challenge, try to do all 3). \n",
    "\n",
    "1. How many times did Shakespeare use the word \"Romeo\"?\n",
    "1. How many distinct words are used?\n",
    "1. What is the average number of times a word is used?\n",
    "1. In version 1, change the file name to a nonexistent file. When does Spark notice that an error has occurred?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
